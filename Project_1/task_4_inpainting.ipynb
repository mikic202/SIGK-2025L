{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2 as cv\n",
    "import torch.optim as optim\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.image.psnr import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_out_region(image: np.ndarray, region_size = 32) -> np.ndarray:\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    region_position_x = random.randint(32, image.shape[1] - region_size - 32)\n",
    "    region_position_y = random.randint(32, image.shape[0] - region_size - 32)\n",
    "    mask[\n",
    "        region_position_y : region_position_y + region_size,\n",
    "        region_position_x : region_position_x + region_size,\n",
    "    ] = 1\n",
    "    return (\n",
    "        image[region_position_y : region_position_y + region_size,\n",
    "        region_position_x : region_position_x + region_size],\n",
    "        mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2InpaintingKDataset(Dataset):\n",
    "    def __init__(self, data, normalize_transform=None):\n",
    "        self.data = data\n",
    "        self.normalize_transform = normalize_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        if self.normalize_transform:\n",
    "            image = self.normalize_transform(image[0])\n",
    "        cutout, mask = cut_out_region(image.permute(1, 2, 0).numpy(), 3)\n",
    "\n",
    "        return image, torch.tensor(cutout).permute([2, 0, 1]), torch.tensor(cv.bitwise_and(image.permute(1, 2, 0).numpy(), image.permute(1, 2, 0).numpy(), mask=(1 - mask.astype(np.uint8)) * 255)).permute([2, 0, 1]), mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 256\n",
    "IMAGE_HIGHT = 256\n",
    "\n",
    "SQUARE_SIZE = 32\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "dataset = datasets.ImageFolder(root=\"/media/mikic202/Nowy/Random/DIV2_dataset/archive/DIV2K_train_HR\")\n",
    "\n",
    "normalize_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_HIGHT, IMAGE_WIDTH)),\n",
    "        # transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_dataset = DIV2InpaintingKDataset(dataset, normalize_transform=normalize_transform)\n",
    "train_dataloader = DataLoader(image_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.relu = nn.LeakyReLU(0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallUNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3, base_ch=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(input_channels, base_ch)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*4)\n",
    "        self.enc3 = ConvBlock(base_ch*4, base_ch*4)\n",
    "        self.enc4 = ConvBlock(base_ch*4, base_ch*8)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "\n",
    "        self.bottleneck1 = ConvBlock(base_ch*16, base_ch*16)\n",
    "        self.bottleneck2 = ConvBlock(base_ch*8, base_ch*8)\n",
    "\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(base_ch*16, base_ch*8, kernel_size=2, stride=2)\n",
    "        self.dec4 = ConvBlock(base_ch*8 + base_ch*8, base_ch*8)   # concat channels\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = ConvBlock(base_ch*4 + base_ch*4, base_ch*4)   # concat channels\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec2 = ConvBlock(base_ch*4 + base_ch*4, base_ch*4)   # concat channels\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*4, base_ch, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec1 = ConvBlock(base_ch + base_ch , base_ch)        # concat channels\n",
    "\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_ch, output_channels, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        # self.activation2 = nn.Sigmoid()\n",
    "\n",
    "        # self.dec0 = ConvBlock(2*output_channels, output_channels)\n",
    "\n",
    "    def bottleneck(self, x):\n",
    "        x = self.bottleneck2(x)\n",
    "        fft_out = torch.fft.rfft2(x, norm=\"ortho\")\n",
    "        freq_processed = self.bottleneck1(torch.cat([fft_out.real, fft_out.imag], dim=1))\n",
    "\n",
    "        c = freq_processed.shape[1] // 2\n",
    "        freq_real = freq_processed[:, :c]\n",
    "        freq_imag = freq_processed[:, c:]\n",
    "\n",
    "        fft_processed = torch.complex(freq_real, freq_imag)\n",
    "        return torch.cat([torch.fft.irfft2(fft_processed, s=x.shape[-2:], norm=\"ortho\"), x], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "\n",
    "        # d0 = torch.nn.functional.interpolate(\n",
    "        #     out, size=x.shape[2:], mode='bilinear', align_corners=False\n",
    "        # )\n",
    "        # out = self.activation2(out)\n",
    "        # d0 = torch.cat([out, x], dim=1)\n",
    "        # d0 = self.dec0(d0)\n",
    "        d0 = self.activation(out)\n",
    "        return d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallUNet(base_ch=40).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"models/really_good_122_only_square_is_blurred_little_upgrade_to_107_sometimes_wierd_color_23.10.2025.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hole_reconstruction_loss(original, reconstructed, mask, ssim = None):\n",
    "    hole = mask.bool().unsqueeze(1)\n",
    "    hole = hole.expand_as(reconstructed)\n",
    "    if ssim is not None:\n",
    "        mask_exp = mask.unsqueeze(1).float().to(reconstructed.device)\n",
    "        return F.l1_loss(reconstructed[hole], original[hole]) + 1 - ssim(reconstructed * mask_exp, original * mask_exp)\n",
    "    return F.l1_loss(reconstructed[hole], original[hole])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.82)\n",
    "sism_criterion = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCHS = 10\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"SIGK-project-1\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUMBER_OF_EPOCHS,\n",
    "        \"loss\": \"L1 + SSIM\",\n",
    "        \"base_ch\": 40,\n",
    "        \"model\": \"SmallUNet with FFT for 3x3\",\n",
    "        \"base\": 122\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "\n",
    "    print(f\"Started Epoch {epoch+1}/{NUMBER_OF_EPOCHS}\")\n",
    "\n",
    "    accumulated_loss = 0.0\n",
    "    mask_loss = 0.0\n",
    "    accumulatesd_ssim = 0.0\n",
    "    accumulated_hole_loss = 0.0\n",
    "    accumulated_criterion_loss = 0.0\n",
    "    accumulated_color_losss = 0.0\n",
    "    for original_images, _, altered_images, mask in train_dataloader:\n",
    "        original_images = original_images.to(device)\n",
    "        altered_images = altered_images.to(device)\n",
    "\n",
    "        reconstructed_images = model(altered_images)\n",
    "        reconstructed_images = torch.nn.functional.interpolate(\n",
    "            reconstructed_images, size=original_images.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "        color_loss = torch.mean(torch.abs(reconstructed_images.mean(dim=[2,3]) - original_images.mean(dim=[2,3])))\n",
    "        criterion_loss = criterion(reconstructed_images, original_images)\n",
    "        holl_loss = hole_reconstruction_loss(original_images, reconstructed_images, mask, sism_criterion)\n",
    "        ssim_value = sism_criterion(reconstructed_images, original_images)\n",
    "        loss = criterion_loss + 2 * holl_loss + 0.8 * (1 - ssim_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "        accumulated_hole_loss += holl_loss.item()\n",
    "        accumulatesd_ssim += ssim_value.item()\n",
    "        accumulated_criterion_loss += criterion_loss.item()\n",
    "        accumulated_color_losss += color_loss.item()\n",
    "\n",
    "    avg_loss = accumulated_loss / len(train_dataloader)\n",
    "    run.log({\"loss\": avg_loss,\n",
    "             \"hole_loss\": accumulated_hole_loss / len(train_dataloader),\n",
    "             \"ssim\": accumulatesd_ssim / len(train_dataloader),\n",
    "             \"criterion_loss\": accumulated_criterion_loss / len(train_dataloader),\n",
    "             \"color_loss\": accumulated_color_losss / len(train_dataloader)\n",
    "            })\n",
    "    print(f\"Loss: {avg_loss:.6f}\")\n",
    "    if epoch % 10 == 0:\n",
    "        scheduler.step()\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model.cpu())\n",
    "model_scripted.save(\n",
    "    \"models/really_good_122_for_3x3_24.10.2025.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_scores(original_image: np.ndarray, altered_image: np.ndarray):\n",
    "  original_image = torch.from_numpy(original_image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "  altered_image = torch.from_numpy(altered_image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "  return {\n",
    "      \"LPIPS\" : LearnedPerceptualImagePatchSimilarity(net_type='squeeze')(original_image, altered_image),\n",
    "      \"PSNR\" : PeakSignalNoiseRatio(1.0)(original_image, altered_image),\n",
    "      \"SSIM\": StructuralSimilarityIndexMeasure()(original_image, altered_image)\n",
    "  }\n",
    "\n",
    "def measure_dataset_difference(original_images: list[np.ndarray], altered_images: list[np.ndarray]):\n",
    "  results = pd.DataFrame([get_image_scores(og, alt) for og, alt in zip(original_images, altered_images)]).astype(float)\n",
    "  return results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(image_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "examples, square, altered_examples, mask = next(iter(test_dataloader))\n",
    "altered_examples = altered_examples.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = model(altered_examples)\n",
    "examples = examples.cpu().numpy()\n",
    "altered_examples = altered_examples.cpu().numpy()\n",
    "reconstructed_examples = reconstructed_images.cpu().numpy()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle(\"VAE Image Reconstruction\", fontsize=16)\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title(\"Original Images\")\n",
    "plt.imshow(examples[0].transpose(1, 2, 0)\n",
    ")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title(\"Altered Images\")\n",
    "plt.imshow(altered_examples[0].transpose(1, 2, 0))\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title(\"Reconstructed Images\")\n",
    "plt.imshow(reconstructed_examples[0].transpose(1, 2, 0))\n",
    "\n",
    "\n",
    "img_np = examples[0].transpose(1, 2, 0)\n",
    "if img_np.max() <= 1.0:\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "else:\n",
    "    img_np = img_np.astype(np.uint8)\n",
    "\n",
    "mask_np = mask.cpu().numpy()[0]\n",
    "mask_np = (mask_np) * 255\n",
    "mask_np = mask_np.astype(np.uint8)\n",
    "\n",
    "img_bgr = cv.cvtColor(img_np, cv.COLOR_RGB2BGR)\n",
    "\n",
    "dst = cv.inpaint(img_bgr, mask_np, 3, cv.INPAINT_TELEA)\n",
    "\n",
    "dst_rgb = cv.cvtColor(dst, cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title(\"OpenCV Inpainted\")\n",
    "plt.imshow(dst_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "only_square_replacement = altered_examples[0].copy() + reconstructed_examples[0].copy() * mask.numpy()[0][np.newaxis, :, :]\n",
    "\n",
    "print(\"Altered vs Reconstructed:\", get_image_scores(examples[0].transpose(1, 2, 0) * 255, only_square_replacement.transpose(1, 2, 0) * 255))\n",
    "print(\"Altered vs OpenCV Inpainted:\", get_image_scores(examples[0].transpose(1, 2, 0) * 255, dst_rgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
